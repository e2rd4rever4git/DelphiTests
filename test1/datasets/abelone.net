FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=0
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (11, 5, 1.00000000000000000000e+000) (11, 5, 1.00000000000000000000e+000) (11, 5, 1.00000000000000000000e+000) (11, 5, 1.00000000000000000000e+000) (11, 5, 1.00000000000000000000e+000) (11, 5, 1.00000000000000000000e+000) (11, 5, 1.00000000000000000000e+000) (11, 5, 1.00000000000000000000e+000) (11, 5, 1.00000000000000000000e+000) (11, 5, 1.00000000000000000000e+000) (0, 5, 1.00000000000000000000e+000) (11, 5, 1.00000000000000000000e+000) (0, 5, 1.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 4.58585565963173860000e-004) (1, -1.04954768755001190000e-001) (2, 1.69980655904206910000e-001) (3, 1.07269207780359290000e+000) (4, -7.48540619074112850000e-001) (5, -3.08480006248225140000e+000) (6, -1.69619153241943340000e+000) (7, 2.07937105502260570000e+000) (8, 3.15716871006644690000e+000) (9, -6.63105709634731370000e-001) (10, 2.09512329632894150000e-001) (0, -2.21740659924183170000e-002) (1, 8.85360289618310900000e-003) (2, 6.48487291511856270000e-002) (3, -6.94806991174088280000e-001) (4, 6.02217620817726780000e-001) (5, 1.49583556875887580000e+000) (6, 5.25461406972099750000e-001) (7, -1.30748375613838120000e+000) (8, -2.03215869122938340000e+000) (9, 2.84438516408818230000e-001) (10, 4.55813413765758510000e-002) (0, 1.09747009639574180000e-002) (1, -9.45086907226147540000e-004) (2, 1.52831007137863570000e-002) (3, -1.40205264743172300000e-001) (4, 1.55423814746285500000e-001) (5, 4.05167036928739290000e-001) (6, 1.65628659944623170000e-001) (7, -2.67463148160970050000e-001) (8, -5.36507436636272320000e-001) (9, 8.06662149344409270000e-002) (10, -1.61045248247646860000e-002) (0, -1.32153824049350750000e-002) (1, -1.69918401165900370000e-002) (2, -9.68523295804446860000e-003) (3, -2.80216404984459720000e-002) (4, 3.17480542592482600000e-002) (5, 8.38673677466815620000e-002) (6, 3.09725772279266650000e-002) (7, -5.92921091603313200000e-002) (8, -1.19201190187896470000e-001) (9, 5.34166555243114490000e-003) (10, 5.02482747018789460000e-003) (0, -6.93389863468186900000e-002) (1, -7.59479366703690870000e-002) (2, -7.49493828342656380000e-002) (3, -7.10309374833656420000e-002) (4, 1.57064990429687080000e-001) (5, 3.05620764511806360000e-001) (6, 1.41144087161395450000e-001) (7, -1.81784290733637390000e-001) (8, -4.79877984361818060000e-001) (9, 1.21807445060029500000e-001) (10, -4.52714159393452840000e-003) (0, -1.38600841209566640000e-001) (1, -4.98748507459024010000e-003) (2, -2.06129835588190400000e-001) (3, 1.21622344487853160000e+000) (4, 3.46142953135419070000e+000) (5, 1.77214129761623340000e+000) (6, 2.96504427617469800000e+000) (7, -9.70312089736636810000e+000) (8, -2.41025584261409210000e+000) (9, 3.58843047902038400000e+000) (10, -7.75429388970264590000e-001) (0, -1.02482412480061900000e-003) (1, -2.57908378790521870000e-003) (2, 1.69114326548867660000e-003) (3, -7.42120657979801620000e-002) (4, 4.55563093968150630000e-002) (5, 1.68132687918474860000e-002) (6, 9.86811664680069080000e-003) (7, -1.76844489645836610000e-002) (8, 2.38670486433019200000e-002) (9, -4.80818477570907550000e-002) (10, 8.41558108306281190000e-003) (0, 1.37385072235362370000e-001) (1, -5.63479801939359700000e-002) (2, -9.61588774131769550000e-002) (3, -8.29838235279556690000e-001) (4, 7.88957568968012170000e-001) (5, 2.54838789874168550000e+000) (6, 1.99435127888969510000e+000) (7, -1.30319019281319170000e+000) (8, -2.58335963523875780000e+000) (9, 8.19778486851274080000e-001) (10, -4.45364320268905880000e-001) (0, -3.84885113216563570000e-002) (1, -3.41427504734762440000e-002) (2, -3.28896352576033490000e-002) (3, 3.95985006420375010000e-002) (4, -9.17160655846688410000e-002) (5, -2.75289824037592170000e-001) (6, -1.40950712758457000000e-001) (7, 1.84560696756083640000e-001) (8, 4.48449242759631270000e-001) (9, -7.39856497545133970000e-002) (10, 3.47175066035205760000e-002) (0, 2.97703872860944630000e-002) (1, -1.72491609676668820000e-001) (2, 6.75744067864858800000e-002) (3, -5.23175154892436600000e-001) (4, 5.60336712283686980000e-001) (5, 1.36721163661460210000e+000) (6, 5.25469587784879620000e-001) (7, -8.83903043594006400000e-001) (8, -1.74324580765870140000e+000) (9, 3.65697415632536330000e-001) (10, -3.56897067152473840000e-002) (11, -3.51022992935556490000e-002) (12, -1.04055718929713810000e-001) (13, -1.46131643289021140000e-002) (14, -5.12324203809993910000e-003) (15, -2.32893081945457480000e-003) (16, 1.52072561128438560000e-001) (17, -3.47546650780127370000e-003) (18, 1.90655000899256720000e-001) (19, 4.69749499085100450000e-003) (20, -5.65863738651722210000e-002) (21, 4.50026944596297220000e-001) 
